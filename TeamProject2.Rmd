---
title: "TeamProject2"
author: "Dance Dance Revolution: Mario Prata Dias Alvarez, Sophie Kate Edelman, Meghan Sarah Hoffman, Elias Florentin John, Kritika Singh, Benjamin Victor Tward"
date: "2025-10-23"
output: 
  html_document:
    toc: True
    toc_float: True
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
library(caret)
#install.packages("recipes")
library(recipes)
```

## Step 0: Why?

Step 0: Context, Objective, and Financial Model

Project Context: Realistic Spotify Metrics

To ground this analysis in real-world performance, we use approximate public and industry figures for a large streaming platform (Q3 2024 estimates):

```{r}
context_data <- data.frame(
  Metric = c("Monthly Active Users (MAU)", "Premium Subscribers", "Average Revenue Per User (ARPU)", "Estimated Monthly Churn Rate", "Customer Lifetime Value (CLV)"),
  Value = c("~600 Million", "~250 Million (42% of MAU)", "~$4.50 / month", "~2.0%", "~$225"),
  Rationale = c("Based on publicly reported figures.", "Based on publicly reported subscription rates.", "Blended rate across different tiers and regions.", "Industry estimate for major streaming services.", "Calculated as ARPU / Churn Rate = $4.50 / 0.02.")
)
colnames(context_data) <- c("Metric", "Estimated Value", "Rationale")
knitr::kable(context_data, caption = "Realistic Spotify Business Metrics", format = "markdown")
```

Objective

Objective: To maximize user retention and daily active listening time, how can Spotify's recommendation engine use a song's audio features (danceability, energy, valence), track_genre, and other variables to build a predictive model that more accurately identifies high-engagement tracks (i.e., high future popularity) for its personalized playlists like 'Discover Weekly'?

Assumption & Financial Model

Assumption: A $10\%$ increase in the number of high-engagement songs a user hears in their 'Discover Weekly' playlist will lead to a very marginal $0.01\%$ decrease in their probability of churning (canceling their subscription) that month.

We translate this into the following Net Financial Impact model to guide the Level 2 Stacker. The model is designed to maximize Net Impact (Revenue - Loss). The $\$$ values reflect the estimated scaled impact on CLV and promotion costs per decision.

```{r}
financial_data <- data.frame(
  Outcome = c("True Positive (TP)", "False Positive (FP)", "False Negative (FN)"),
  Metric = c("Correctly identifies a Hidden Gem (Target = 'Yes').", "Predicts 'Yes', but actual is 'No'. (Wasteful promotion)", "Predicts 'No', but actual is 'Yes'. (Lost opportunity)"),
  Financial_Value = c("+$200", "-$500", "-$100"),
  Financial_Rationale = c("Revenue: Estimated net monthly value derived from the 0.01% reduction in churn for a high-value, retained customer. (Maximize)", "Cost: Significant loss from license fees, content storage, and opportunity cost of promoting non-engaging content that accelerates churn. (Minimize - Critical Error)", "Cost: Minor loss from missing a viable track and losing the opportunity for increased engagement and retention. (Minimize - Minor Error)")
)
colnames(financial_data) <- c("Outcome", "Metric", "Financial Value", "Financial Rationale")
knitr::kable(financial_data, caption = "Net Financial Impact Model (Per Prediction)", format = "markdown")
```

The objective is to maximize the Net Impact, calculated as:

\text{Net Impact} = (\text{TP} \times \$200) - (\text{FP} \times \$500) - (\text{FN} \times \$100)
## Stacked Model Architecture

The process uses a **6-model Level 1 (L1) Ensemble** feeding into a **Level 2 (L2) Stacker**.

| Level | Model Name | Role | Optimization |
| :--- | :--- | :--- | :--- |
| **Level 1** | LR, KNN, ANN, RF, DT, SVM | **Specialists:** Trained on raw features (danceability, genre, etc.) to capture diverse linear and non-linear patterns. | Standard statistical metrics (Kappa). |
| **Level 2** | Cost-Sensitive Logistic Regression | **Producer:** Trained on L1 probability outputs using **weighted learning** based on the $5:1$ loss ratio ($-\$500$ FP cost vs. $-\$100$ FN cost). | Maximize Net Financial Impact by aggressively minimizing False Positives. |


## Step 1: Load the Data

```{r}
songs <- read.csv("Spotify.csv", stringsAsFactors = T)
```

## Step 2: Clean the Data

```{r}
songs = songs |>
  select(-c(X, track_id, album_name, track_name, artists)) |>
  mutate(popularity = ifelse(popularity > 50, 1, 0))
# If we wanted to incorporate artist popularity, we would not delete artists
# Approach would be GEE with artist groupings, gaussian family for linear outcome (not fully robust but acceptable)
# For now, we can look at naive approach to consider all artists to be equal playing field

songs |> glimpse()
```

## Step 3: Split the Data

```{r}
n = nrow(songs)
p = .7

set.seed(12345)
ind = sample(1:n, n*p)
train = songs[ind,]
test = songs[-ind,]

rec = recipe(popularity ~ ., data = train) |>
  step_impute_median(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

prep_rec = prep(rec, training = train, retain = FALSE)

X_train = bake(prep_rec, new_data = train, all_predictors())
X_test  = bake(prep_rec, new_data = test, all_predictors())

y_train = train$popularity
y_test  = test$popularity

train = X_train |> mutate(popularity = y_train)
test = X_test |> mutate(popularity = y_test)

```

## Step 4, 5, and 6: The Models

### Logistic Regression

#### Step 4: Build the Model

```{r}
m1 <- glm(popularity ~ ., data = train, family=binomial())
```

#### Step 5: Predict

```{r}
song_pred_log <- predict(m1, test)
song_pred_bin <- ifelse(song_pred_log >= .5, 1, 0)
song_test_bin <- test$popularity

```

#### Step 6: Evaluate the Model

```{r}
lm_confusion <- confusionMatrix(as.factor(song_pred_bin), as.factor(song_test_bin), positive = "1")

lm_confusion
```

**Accuracy:** Our Linear Regression model predicted whether or not a song was popular about `r round((as.numeric(lm_confusion$overall["Accuracy"]) * 100), digits = 2)`% of the time.

**Sensitivity:** Of all the song that were popular, our model identified about `r round((as.numeric(lm_confusion$byClass["Sensitivity"]) * 100), digits = 2)`% of them.

**Specificity:** Of all the songs that were not popular, our model identified about `r round((as.numeric(lm_confusion$byClass["Specificity"]) * 100), digits = 2)`% of them.

**What we changed to get the best results**

### KNN

#### Step 4: Build the Model

#### Step 5: Predict

#### Step 6: Evaluate the Model

**What we changed to get the best results**

### Decision Tree (With and without Cost Matrix)

#### Step 4: Build the Model

```{r, cache=TRUE}
library(C50)
decision_m1 <- C5.0(as.factor(popularity) ~., data = train)
```

To try to improve the accuracy of the model, we are also going to implement a cost matrix.

```{r}
cost_matrix <- matrix(c( 0, 1, 3, 0), nrow = 2)
cost_matrix
```

And build a model with the cost matrix.

```{r, cache=TRUE}
decision_cost_m1 <- C5.0(as.factor(popularity) ~., data = train, costs = cost_matrix)
```

#### Step 5: Predict

After training the Decision Tree models (with and without the cost matrix), we then had them predict values for the test data.![]()

```{r}
decision_pop_cost_pred <- predict(decision_cost_m1, test)
decision_pop_pred <- predict(decision_m1, test)
summary(decision_pop_cost_pred)
summary(decision_pop_pred)
```

#### Step 6: Evaluate the Model

To understand the effectiveness of the Decision Tree model [without the cost matrix]{.underline}, we then build a confusion matrix where we compared the number of songs we predicted were going to be popular to the actual number of songs that were popular.

```{r}
library(caret)
decision_confusion <- confusionMatrix(as.factor(decision_pop_pred), as.factor(test$popularity), positive = "1")
decision_sensitivity <- as.numeric(decision_confusion$byClass["Sensitivity"])
decision_confusion

```

**Accuracy:** Our Decision Tree model predicted whether or not a song was popular about `r round((as.numeric(decision_confusion$overall["Accuracy"]) * 100), digits = 2)`% of the time.

**Sensitivity:** Of all the song that were popular, our model identified about `r round((as.numeric(decision_confusion$byClass["Sensitivity"]) * 100), digits = 2)`% of them.

**Specificity:** Of all the songs that were not popular, our model identified about `r round((as.numeric(decision_confusion$byClass["Specificity"]) * 100), digits = 2)`% of them.

[**However if we add the cost_matrix to the model...**]{.underline}

To understand the effectiveness of the decision tree model with the cost matrix, we then build a confusion matrix where we compared the number of songs we predicted were going to be populart to the actual number of songs that were popular.

```{r}
library(caret)
decision_confusion_cost <- confusionMatrix(as.factor(decision_pop_cost_pred), as.factor(test$popularity), positive = "1")
decision_sensitivity_cost <- as.numeric(decision_confusion_cost$byClass["Sensitivity"])
decision_confusion_cost
```

**Accuracy:** Our Decision Tree model with a cost matrix predicted whether or not a song was popular about `r round((as.numeric(decision_confusion_cost$overall["Accuracy"]) * 100), digits = 2)`% of the time.

**Sensitivity:** Of all the song that were popular, our model identified about `r round((as.numeric(decision_confusion_cost$byClass["Sensitivity"]) * 100), digits = 2)`% of them.

**Specificity:** Of all the songs that were not popular, our model identified about `r round((as.numeric(decision_confusion_cost$byClass["Specificity"]) * 100), digits = 2)`% of them.

**What we changed to get the best results**

### ANN

#### Step 4: Build the Model

#### Step 5: Predict

#### Step 6: Evaluate the Model

**What we changed to get the best results**

### SVM

#### Step 4: Build the Model

#### Step 5: Predict

#### Step 6: Evaluate the Model

**What we changed to get the best results**

## Step 7: Conclusion

### Combined Models

### Overall Conclusion

### Possible Errors
