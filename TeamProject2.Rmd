---
title: "TeamProject2"
author: "Dance Dance Revolution: Mario Prata Dias Alvarez, Sophie Kate Edelman, Meghan Sarah Hoffman, Elias Florentin John, Kritika Singh, Benjamin Victor Tward"
date: "2025-10-23"
output: 
  html_document:
    toc: True
    toc_float: True
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidyverse")
library(tidyverse)
library(caret)
#install.packages("recipes")
library(recipes)
```

## Step 0: Why?

**Objective:** To maximize user retention and daily active listening time, how can Spotify's recommendation engine use a song's audio features (danceability, energy, valence), track_genre, and other variables to build a predictive model that more accurately identifies high-engagement tracks (i.e., high future popularity) for its personalized playlists like 'Discover Weekly'?

**Assumption:** A 10% increase in the number of high-engagement songs a user hears in their 'Discover Weekly' playlist will lead to a 0.5% decrease in their probability of churning (canceling their subscription) that month.

## Step 1: Load the Data

```{r}
songs <- read.csv("Spotify.csv", stringsAsFactors = T)
```

## Step 2: Clean the Data

```{r}
songs = songs |>
  select(-c(X, track_id, album_name, track_name, artists)) |>
  mutate(popularity = ifelse(popularity > 50, 1, 0))
# If we wanted to incorporate artist popularity, we would not delete artists
# Approach would be GEE with artist groupings, gaussian family for linear outcome (not fully robust but acceptable)
# For now, we can look at naive approach to consider all artists to be equal playing field

songs |> glimpse()
```

## Step 3: Split the Data

```{r}
n = nrow(songs)
p = .7

set.seed(12345)
ind = sample(1:n, n*p)
train = songs[ind,]
test = songs[-ind,]

rec = recipe(popularity ~ ., data = train) |>
  step_impute_median(all_numeric_predictors()) |> 
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

prep_rec = prep(rec, training = train, retain = FALSE)

X_train = bake(prep_rec, new_data = train, all_predictors())
X_test  = bake(prep_rec, new_data = test, all_predictors())

y_train = train$popularity
y_test  = test$popularity

train = X_train |> mutate(popularity = y_train)
test = X_test |> mutate(popularity = y_test)

```

## Step 4, 5, and 6: The Models

### Logistic Regression

#### Step 4: Build the Model

```{r}
m1 <- glm(popularity ~ ., data = train, family=binomial())
```

#### Step 5: Predict

```{r}
song_pred_log <- predict(m1, test)
song_pred_bin <- ifelse(song_pred_log >= .5, 1, 0)
song_test_bin <- test$popularity

```

#### Step 6: Evaluate the Model

```{r}
lm_confusion <- confusionMatrix(as.factor(song_pred_bin), as.factor(song_test_bin), positive = "1")

lm_confusion
```

**Accuracy:** Our Linear Regression model predicted whether or not a song was popular about `r round((as.numeric(lm_confusion$overall["Accuracy"]) * 100), digits = 2)`% of the time.

**Sensitivity:** Of all the song that were popular, our model identified about `r round((as.numeric(lm_confusion$byClass["Sensitivity"]) * 100), digits = 2)`% of them.

**Specificity:** Of all the songs that were not popular, our model identified about `r round((as.numeric(lm_confusion$byClass["Specificity"]) * 100), digits = 2)`% of them.

**What we changed to get the best results**

### KNN

#### Step 4: Build the Model

#### Step 5: Predict

#### Step 6: Evaluate the Model

**What we changed to get the best results**

### Decision Tree (With and without Cost Matrix)

#### Step 4: Build the Model

```{r, cache=TRUE}
library(C50)
decision_m1 <- C5.0(as.factor(popularity) ~., data = train)
```

To try to improve the accuracy of the model, we are also going to implement a cost matrix.

```{r}
cost_matrix <- matrix(c( 0, 1, 3, 0), nrow = 2)
cost_matrix
```

And build a model with the cost matrix.

```{r, cache=TRUE}
decision_cost_m1 <- C5.0(as.factor(popularity) ~., data = train, costs = cost_matrix)
```

#### Step 5: Predict

After training the Decision Tree models (with and without the cost matrix), we then had them predict values for the test data.![]()

```{r}
decision_pop_cost_pred <- predict(decision_cost_m1, test)
decision_pop_pred <- predict(decision_m1, test)
summary(decision_pop_cost_pred)
summary(decision_pop_pred)
```

#### Step 6: Evaluate the Model

To understand the effectiveness of the Decision Tree model [without the cost matrix]{.underline}, we then build a confusion matrix where we compared the number of songs we predicted were going to be popular to the actual number of songs that were popular.

```{r}
library(caret)
decision_confusion <- confusionMatrix(as.factor(decision_pop_pred), as.factor(test$popularity), positive = "1")
decision_sensitivity <- as.numeric(decision_confusion$byClass["Sensitivity"])
decision_confusion

```

**Accuracy:** Our Decision Tree model predicted whether or not a song was popular about `r round((as.numeric(decision_confusion$overall["Accuracy"]) * 100), digits = 2)`% of the time.

**Sensitivity:** Of all the song that were popular, our model identified about `r round((as.numeric(decision_confusion$byClass["Sensitivity"]) * 100), digits = 2)`% of them.

**Specificity:** Of all the songs that were not popular, our model identified about `r round((as.numeric(decision_confusion$byClass["Specificity"]) * 100), digits = 2)`% of them.

[**However if we add the cost_matrix to the model...**]{.underline}

To understand the effectiveness of the decision tree model with the cost matrix, we then build a confusion matrix where we compared the number of songs we predicted were going to be populart to the actual number of songs that were popular.

```{r}
library(caret)
decision_confusion_cost <- confusionMatrix(as.factor(decision_pop_cost_pred), as.factor(test$popularity), positive = "1")
decision_sensitivity_cost <- as.numeric(decision_confusion_cost$byClass["Sensitivity"])
decision_confusion_cost
```

**Accuracy:** Our Decision Tree model with a cost matrix predicted whether or not a song was popular about `r round((as.numeric(decision_confusion_cost$overall["Accuracy"]) * 100), digits = 2)`% of the time.

**Sensitivity:** Of all the song that were popular, our model identified about `r round((as.numeric(decision_confusion_cost$byClass["Sensitivity"]) * 100), digits = 2)`% of them.

**Specificity:** Of all the songs that were not popular, our model identified about `r round((as.numeric(decision_confusion_cost$byClass["Specificity"]) * 100), digits = 2)`% of them.

**What we changed to get the best results**

### ANN

#### Step 4: Build the Model

#### Step 5: Predict

#### Step 6: Evaluate the Model

**What we changed to get the best results**

## Step 7: Conclusion

### Combined Models

### Overall Conclusion

### Possible Errors
